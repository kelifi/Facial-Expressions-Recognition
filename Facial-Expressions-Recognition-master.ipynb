{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f873ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cbee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "img_rows,img_cols = 48,48\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b712947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "train_data_dir='C:/Users/21653/Desktop/face-expression-recognition-dataset/face-expression-recognition-dataset/train'\n",
    "validation_data_dir='C:/Users/21653/Desktop/face-expression-recognition-dataset/face-expression-recognition-dataset/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a24b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation data\n",
    "train_datagen = ImageDataGenerator(\n",
    "\t\t\t\t\trescale=1./255,\n",
    "\t\t\t\t\trotation_range=30,\n",
    "\t\t\t\t\tshear_range=0.3,\n",
    "\t\t\t\t\tzoom_range=0.3,\n",
    "\t\t\t\t\twidth_shift_range=0.4,\n",
    "\t\t\t\t\theight_shift_range=0.4,\n",
    "\t\t\t\t\thorizontal_flip=True,\n",
    "\t\t\t\t\tfill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057fb073",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ee9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24282 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "\t\t\t\t\ttrain_data_dir,\n",
    "\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\tshuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114879b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5937 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\t\t\t\t\t\t\tvalidation_data_dir,\n",
    "\t\t\t\t\t\t\tcolor_mode='grayscale',\n",
    "\t\t\t\t\t\t\ttarget_size=(img_rows,img_cols),\n",
    "\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\tclass_mode='categorical',\n",
    "\t\t\t\t\t\t\tshuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97917fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bea5678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e97027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6783db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,037\n",
      "Trainable params: 1,325,861\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BUILDING MODEL\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Block-1\n",
    "\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-2 \n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-3\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-4 \n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block-5\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-6\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block-7\n",
    "\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090f38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint('Emotion_little_vgg.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a4bd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-c19629e8796c>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.8388 - accuracy: 0.2393\n",
      "Epoch 00001: val_loss improved from inf to 1.54811, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 728s 964ms/step - loss: 1.8388 - accuracy: 0.2393 - val_loss: 1.5481 - val_accuracy: 0.3054\n",
      "Epoch 2/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.5747 - accuracy: 0.2824\n",
      "Epoch 00002: val_loss improved from 1.54811 to 1.52318, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 661s 876ms/step - loss: 1.5747 - accuracy: 0.2824 - val_loss: 1.5232 - val_accuracy: 0.3256\n",
      "Epoch 3/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.5523 - accuracy: 0.2964\n",
      "Epoch 00003: val_loss improved from 1.52318 to 1.49994, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 661s 875ms/step - loss: 1.5523 - accuracy: 0.2964 - val_loss: 1.4999 - val_accuracy: 0.3515\n",
      "Epoch 4/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.5254 - accuracy: 0.3143\n",
      "Epoch 00004: val_loss improved from 1.49994 to 1.44744, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 660s 874ms/step - loss: 1.5254 - accuracy: 0.3143 - val_loss: 1.4474 - val_accuracy: 0.3595\n",
      "Epoch 5/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.4760 - accuracy: 0.3436\n",
      "Epoch 00005: val_loss improved from 1.44744 to 1.30106, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 712s 943ms/step - loss: 1.4760 - accuracy: 0.3436 - val_loss: 1.3011 - val_accuracy: 0.4405\n",
      "Epoch 6/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3945 - accuracy: 0.4020\n",
      "Epoch 00006: val_loss improved from 1.30106 to 1.12092, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 802s 1s/step - loss: 1.3945 - accuracy: 0.4020 - val_loss: 1.1209 - val_accuracy: 0.5390\n",
      "Epoch 7/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.3067 - accuracy: 0.4510\n",
      "Epoch 00007: val_loss improved from 1.12092 to 1.06800, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 845s 1s/step - loss: 1.3067 - accuracy: 0.4510 - val_loss: 1.0680 - val_accuracy: 0.5696\n",
      "Epoch 8/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.2419 - accuracy: 0.4828\n",
      "Epoch 00008: val_loss improved from 1.06800 to 0.95752, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 873s 1s/step - loss: 1.2419 - accuracy: 0.4828 - val_loss: 0.9575 - val_accuracy: 0.6159\n",
      "Epoch 9/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.1956 - accuracy: 0.5075\n",
      "Epoch 00009: val_loss improved from 0.95752 to 0.89032, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 790s 1s/step - loss: 1.1956 - accuracy: 0.5075 - val_loss: 0.8903 - val_accuracy: 0.6391\n",
      "Epoch 10/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.1581 - accuracy: 0.5272\n",
      "Epoch 00010: val_loss did not improve from 0.89032\n",
      "755/755 [==============================] - 774s 1s/step - loss: 1.1581 - accuracy: 0.5272 - val_loss: 0.9600 - val_accuracy: 0.6079\n",
      "Epoch 11/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.1379 - accuracy: 0.5416\n",
      "Epoch 00011: val_loss improved from 0.89032 to 0.87162, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 683s 905ms/step - loss: 1.1379 - accuracy: 0.5416 - val_loss: 0.8716 - val_accuracy: 0.6515\n",
      "Epoch 12/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.1186 - accuracy: 0.5555\n",
      "Epoch 00012: val_loss improved from 0.87162 to 0.84435, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 667s 883ms/step - loss: 1.1186 - accuracy: 0.5555 - val_loss: 0.8444 - val_accuracy: 0.6650\n",
      "Epoch 13/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0995 - accuracy: 0.5607\n",
      "Epoch 00013: val_loss did not improve from 0.84435\n",
      "755/755 [==============================] - 667s 883ms/step - loss: 1.0995 - accuracy: 0.5607 - val_loss: 0.9096 - val_accuracy: 0.6398\n",
      "Epoch 14/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0830 - accuracy: 0.5691\n",
      "Epoch 00014: val_loss did not improve from 0.84435\n",
      "755/755 [==============================] - 673s 892ms/step - loss: 1.0830 - accuracy: 0.5691 - val_loss: 0.8716 - val_accuracy: 0.6610\n",
      "Epoch 15/25\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0696 - accuracy: 0.5770Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.84435\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "755/755 [==============================] - 790s 1s/step - loss: 1.0696 - accuracy: 0.5770 - val_loss: 0.9272 - val_accuracy: 0.6405\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "nb_train_samples = 24172\n",
    "nb_validation_samples = 3006\n",
    "epochs=25\n",
    "\n",
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a84b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0727 - accuracy: 0.5759\n",
      "Epoch 00001: val_loss improved from 0.84435 to 0.80980, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 867s 1s/step - loss: 1.0727 - accuracy: 0.5759 - val_loss: 0.8098 - val_accuracy: 0.6747\n",
      "Epoch 2/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0526 - accuracy: 0.5818\n",
      "Epoch 00002: val_loss did not improve from 0.80980\n",
      "755/755 [==============================] - 861s 1s/step - loss: 1.0526 - accuracy: 0.5818 - val_loss: 0.8115 - val_accuracy: 0.6794\n",
      "Epoch 3/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0522 - accuracy: 0.5823\n",
      "Epoch 00003: val_loss improved from 0.80980 to 0.79518, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 867s 1s/step - loss: 1.0522 - accuracy: 0.5823 - val_loss: 0.7952 - val_accuracy: 0.6895\n",
      "Epoch 4/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0354 - accuracy: 0.5919\n",
      "Epoch 00004: val_loss improved from 0.79518 to 0.79489, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 868s 1s/step - loss: 1.0354 - accuracy: 0.5919 - val_loss: 0.7949 - val_accuracy: 0.6878\n",
      "Epoch 5/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0338 - accuracy: 0.5937\n",
      "Epoch 00005: val_loss did not improve from 0.79489\n",
      "755/755 [==============================] - 863s 1s/step - loss: 1.0338 - accuracy: 0.5937 - val_loss: 0.7982 - val_accuracy: 0.6929\n",
      "Epoch 6/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0242 - accuracy: 0.6009\n",
      "Epoch 00006: val_loss did not improve from 0.79489\n",
      "755/755 [==============================] - 864s 1s/step - loss: 1.0242 - accuracy: 0.6009 - val_loss: 0.8056 - val_accuracy: 0.6892\n",
      "Epoch 7/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0168 - accuracy: 0.5985\n",
      "Epoch 00007: val_loss improved from 0.79489 to 0.78535, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 807s 1s/step - loss: 1.0168 - accuracy: 0.5985 - val_loss: 0.7854 - val_accuracy: 0.6922\n",
      "Epoch 8/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0224 - accuracy: 0.6004\n",
      "Epoch 00008: val_loss did not improve from 0.78535\n",
      "755/755 [==============================] - 681s 902ms/step - loss: 1.0224 - accuracy: 0.6004 - val_loss: 0.7859 - val_accuracy: 0.6949\n",
      "Epoch 9/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0048 - accuracy: 0.6052\n",
      "Epoch 00009: val_loss improved from 0.78535 to 0.77019, saving model to Emotion_little_vgg.h5\n",
      "755/755 [==============================] - 679s 899ms/step - loss: 1.0048 - accuracy: 0.6052 - val_loss: 0.7702 - val_accuracy: 0.7103\n",
      "Epoch 10/10\n",
      "755/755 [==============================] - ETA: 0s - loss: 1.0118 - accuracy: 0.6013\n",
      "Epoch 00010: val_loss did not improve from 0.77019\n",
      "755/755 [==============================] - 681s 901ms/step - loss: 1.0118 - accuracy: 0.6013 - val_loss: 0.7768 - val_accuracy: 0.7050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=10,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving our model\n",
    "model.save('Facial-Expressions-Recognition-master2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2e036d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face detection\n",
    "face_classifier=cv2.CascadeClassifier('C:/Users/21653/Desktop/Facial-Expressions-Recognition-master/Facial-Expressions-Recognition-master/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bf3bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of labels \n",
    "class_labels = ['Angry','Happy','Neutral','Sad','Surprise']\n",
    "cap = cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07b48290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Facial-Expressions-Recognition-master2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Facial-Expressions-Recognition-master2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d9bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378249e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9e147c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb36cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9433b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "    # rect,face,image = face_detector(frame)\n",
    "\n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = model.predict(roi)[0]\n",
    "            label=class_labels[preds.argmax()]\n",
    "            label_position = (x,y)\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "    cv2.imshow('Emotion Detector',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f66bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
